{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca1ff0a-23ef-4c69-a1b0-450c70c5396b",
   "metadata": {},
   "source": [
    "# Hands-on Exercise: Processing Legacy Lab Data for ML\n",
    "\n",
    "## Duration: 90 minutes\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this exercise, you'll learn how to: \n",
    "1. Clean and standardize heterogeneous lab data \n",
    "2. Merge data from multiple sources\n",
    "3. Handle missing values and inconsistencies\n",
    "4. Prepare data for ML model training\n",
    "5. Build a basic ML model for antibody property prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85210b1-cbe8-45f4-bea2-fae05052bfe8",
   "metadata": {},
   "source": [
    "### Feature Engineering and ML Preparation\n",
    "\n",
    "You should start by cleaning the data using the `antibody_data_preprocessing` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c42526-5999-4eca-b776-b4fa7f94b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_ml(merged_df):\n",
    "    \"\"\"Prepare merged data for ML modeling\"\"\"\n",
    "    # Create feature matrix\n",
    "    features = [\n",
    "        'binding_affinity_kd',\n",
    "        'thermostability_tm1_celsius',\n",
    "        'asec_monomerpct'\n",
    "    ]\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = merged_df[features].copy()\n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    # Create a simple target variable (should be customized).\n",
    "    # Here we're creating a \"quality score\" combining multiple properties,\n",
    "    # quantifying whether a given sequence is better than the median\n",
    "    y = (\n",
    "        (X['thermostability_tm1_celsius'] > X['thermostability_tm1_celsius'].median()).astype(int) + \n",
    "        (X['asec_monomerpct'] > X['asec_monomerpct'].median()).astype(int) +\n",
    "        (X['binding_affinity_kd'] < X['binding_affinity_kd'].median()).astype(int)\n",
    "    )\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4e6de-24ff-4e3c-a2d2-4214abf7e610",
   "metadata": {},
   "source": [
    "### Build an LSTM to predict the target variable from antibody sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f9d02d-56e3-4e35-8ec7-032f67f8faef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import List, Tuple\n",
    "\n",
    "class AntibodyDataset(Dataset):\n",
    "    def __init__(self, merged_experimental_df: pd.DataFrame):\n",
    "        self.data = merged_experimental_df\n",
    "        #sequences_df.merge(experimental_df, on='Sample ID')\n",
    "\n",
    "        # Create amino acid vocabulary\n",
    "        self.aa_vocab = {aa: idx for idx, aa in enumerate('ACDEFGHIKLMNPQRSTVWY')}\n",
    "'''\n",
    "        # Standardize experimental features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.exp_features = ['KD (nM)', 'Tm1', 'Tm2', '% POI']\n",
    "        self.exp_data = self.scaler.fit_transform(self.data[self.exp_features])\n",
    "'''\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Convert sequences to numerical arrays\n",
    "        vh_tensor = torch.tensor([self.aa_vocab[aa] for aa in row['sequences_hc_sequence']], dtype=torch.long)\n",
    "        vl_tensor = torch.tensor([self.aa_vocab[aa] for aa in row['sequences_lc_sequence']], dtype=torch.long)\n",
    "\n",
    "        # Get experimental features\n",
    "      #  exp_tensor = torch.tensor(self.exp_data[idx], dtype=torch.float)\n",
    "\n",
    "        # Get target variables\n",
    "        y = prepare_for_ml(self.data)[1]\n",
    "\n",
    "        return {\n",
    "            'antibody_id': row['antibody_id'],\n",
    "            'vh': vh_tensor,\n",
    "            'vl': vl_tensor,\n",
    "            'target': y\n",
    "        }\n",
    "\n",
    "class AntibodyPropertyPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size: int = 20, embedding_dim: int = 32,\n",
    "                 hidden_dim: int = 64, num_exp_features: int = 4):\n",
    "        super().__init__()\n",
    "\n",
    "        # Sequence processing\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.vh_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.vl_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Experimental feature processing\n",
    "        self.exp_linear = nn.Linear(num_exp_features, hidden_dim)\n",
    "\n",
    "        # Combined processing\n",
    "        self.combine_layer = nn.Linear(hidden_dim * 4 + hidden_dim, hidden_dim)\n",
    "\n",
    "        # Output layers\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, vh, vl):\n",
    "        # Process VH sequence\n",
    "        vh_emb = self.embedding(vh)\n",
    "        vh_out, (vh_hidden, _) = self.vh_lstm(vh_emb)\n",
    "        vh_feat = torch.cat((vh_hidden[-2,:,:], vh_hidden[-1,:,:]), dim=1)\n",
    "\n",
    "        # Process VL sequence\n",
    "        vl_emb = self.embedding(vl)\n",
    "        vl_out, (vl_hidden, _) = self.vl_lstm(vl_emb)\n",
    "        vl_feat = torch.cat((vl_hidden[-2,:,:], vl_hidden[-1,:,:]), dim=1)\n",
    "\n",
    "        # Process experimental features\n",
    "       # exp_feat = self.exp_linear(exp_features)\n",
    "\n",
    "        # Combine all features\n",
    "        combined = torch.cat([vh_feat, vl_feat], dim=1)\n",
    "        hidden = F.relu(self.combine_layer(combined))\n",
    "\n",
    "        # Generate prediction\n",
    "        output = self.output_layer(hidden)\n",
    "        return output\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader,\n",
    "                num_epochs: int = 100, learning_rate: float = 0.001):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(batch['vh'], batch['vl'])\n",
    "            loss = criterion(output, batch['target'].unsqueeze(1))\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return train_losses\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> Tuple[List[float], List[float]]:\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            output = model(batch['vh'], batch['vl'], batch['exp_features'])\n",
    "            predictions.extend(output.squeeze().tolist())\n",
    "            actuals.extend(batch['target'].tolist())\n",
    "\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25646ce72d574c46",
   "metadata": {},
   "source": [
    "### Train the model on the example data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa6952163908cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    seq_df = pd.read_csv('antibody_sequences.csv')\n",
    "    aggregation_df = pd.read_csv('asec_data.csv')\n",
    "    binding_df = pd.read_csv('binding_data.csv')\n",
    "    stability_df = pd.read_csv('stability_data.csv')\n",
    "\n",
    "    aggregation_df = clean_aggregation_data(aggregation_df)\n",
    "    binding_df = clean_binding_data(binding_df)\n",
    "    stability_df = clean_stability_data(stability_df)\n",
    "\n",
    "    integrated_dataset = integrate_datasets(binding_df, stability_df, aggregation_df)\n",
    "    targets_df = prepare_for_ml(integrated_dataset)[0]\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = AntibodyDataset(seq_df, targets_df)\n",
    "\n",
    "    # Split dataset\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = AntibodyTransformer(\n",
    "        vocab_size=20,\n",
    "        d_model=128,\n",
    "        nhead=8,\n",
    "        num_layers=3,\n",
    "        dropout=0.1\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    train_model(model, train_loader, val_loader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621c034-f61e-45ef-8bb6-34c814e1d4f5",
   "metadata": {},
   "source": [
    "# Transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5731c-0b23-405e-ba69-e627058b26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AntibodyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=20, d_model=128, nhead=8, num_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer for amino acid sequences\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=512,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer=self.pos_encoder,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = nn.Linear(d_model * 2, 256)  # *2 because we have VH and VL\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 4)  # 3 outputs: aggregation, KD, Tm1\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 2, seq_length]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Process VH and VL sequences separately\n",
    "        vh = x[:, 0, :]  # [batch_size, seq_length]\n",
    "        vl = x[:, 1, :]  # [batch_size, seq_length]\n",
    "        \n",
    "        # Embed sequences\n",
    "        vh_embedded = self.embedding(vh).transpose(0, 1)  # [seq_length, batch_size, d_model]\n",
    "        vl_embedded = self.embedding(vl).transpose(0, 1)  # [seq_length, batch_size, d_model]\n",
    "        \n",
    "        # Pass through transformer\n",
    "        vh_encoded = self.transformer(vh_embedded)  # [seq_length, batch_size, d_model]\n",
    "        vl_encoded = self.transformer(vl_embedded)  # [seq_length, batch_size, d_model]\n",
    "        \n",
    "        # Pool sequence dimension\n",
    "        vh_pooled = vh_encoded.mean(dim=0)  # [batch_size, d_model]\n",
    "        vl_pooled = vl_encoded.mean(dim=0)  # [batch_size, d_model]\n",
    "        \n",
    "        # Concatenate VH and VL features\n",
    "        combined = torch.cat([vh_pooled, vl_pooled], dim=1)\n",
    "        \n",
    "        # Final MLP layers\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=1e-4):\n",
    "    \"\"\"Training loop with validation\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for sequences, targets in train_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in val_loader:\n",
    "                sequences = sequences.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e05780-61e3-49c7-8b7c-4662fd374443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load data\n",
    "    seq_df = pd.read_csv('antibody_sequences.csv')\n",
    "    asec_df = pd.read_csv('asec_data.csv')\n",
    "    binding_df = pd.read_csv('binding_data.csv')\n",
    "    stability_df = pd.read_csv('stability_data.csv')\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = AntibodyDataset(seq_df, asec_df, binding_df, stability_df)\n",
    "    \n",
    "    # Split dataset\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = AntibodyTransformer(\n",
    "        vocab_size=20,\n",
    "        d_model=128,\n",
    "        nhead=8,\n",
    "        num_layers=3,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    train_model(model, train_loader, val_loader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059b624-08f1-4954-a801-b4f2c70e9607",
   "metadata": {},
   "source": [
    "### Exercise Tasks\n",
    "\n",
    "1.  Data Loading and Assessment\n",
    "    -   Load all three data files\n",
    "    -   Run initial quality assessment\n",
    "    -   Identify key data issues\n",
    "2.  Data Cleaning\n",
    "    -   Implement temperature standardization\n",
    "    -   Clean binding affinity values\n",
    "    -   Handle missing values\n",
    "    -   Standardize units\n",
    "3.  Data Integration\n",
    "    -   Merge datasets\n",
    "    -   Handle duplicate measurements\n",
    "    -   Create final feature matrix\n",
    "4.  ML Model Development\n",
    "    -   Create feature matrix\n",
    "    -   Define target variable\n",
    "    -   Train simple model\n",
    "    -   Evaluate results\n",
    "\n",
    "### Bonus Challenges\n",
    "\n",
    "1.  Advanced Data Cleaning\n",
    "    -   Implement outlier detection\n",
    "    -   Add data validation rules\n",
    "    -   Create data quality reports\n",
    "2.  Feature Engineering\n",
    "    -   Create derived features\n",
    "    -   Implement domain-specific transformations\n",
    "    -   Add sequence-based features\n",
    "3.  Model Improvements\n",
    "    -   Implement cross-validation\n",
    "    -   Try different ML algorithms\n",
    "    -   Add uncertainty quantification\n",
    "\n",
    "### Tips for Success\n",
    "\n",
    "1.  Data Quality\n",
    "    -   Always plot your data\n",
    "    -   Check for outliers\n",
    "    -   Validate units\n",
    "    -   Document assumptions\n",
    "2.  Integration\n",
    "    -   Verify sample IDs\n",
    "    -   Check for duplicates\n",
    "    -   Validate merged data\n",
    "3.  ML Development\n",
    "    -   Start simple\n",
    "    -   Test assumptions\n",
    "    -   Validate results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
