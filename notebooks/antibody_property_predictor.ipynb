{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca1ff0a-23ef-4c69-a1b0-450c70c5396b",
   "metadata": {},
   "source": [
    "# Hands-on Exercise: Processing Legacy Lab Data for ML\n",
    "\n",
    "## Duration: 90 minutes\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this exercise, you'll learn how to: \n",
    "1. Clean and standardize heterogeneous lab data \n",
    "2. Merge data from multiple sources\n",
    "3. Handle missing values and inconsistencies\n",
    "4. Prepare data for ML model training\n",
    "5. Build a basic transformer for antibody property prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85210b1-cbe8-45f4-bea2-fae05052bfe8",
   "metadata": {},
   "source": [
    "### Feature Engineering and ML Preparation\n",
    "\n",
    "You should start by cleaning the data using the `antibody_data_preprocessing` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621c034-f61e-45ef-8bb6-34c814e1d4f5",
   "metadata": {},
   "source": [
    "# Transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3e5731c-0b23-405e-ba69-e627058b26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import List, Tuple\n",
    "\n",
    "class AntibodyDataset(Dataset):\n",
    "    def __init__(self, merged_experimental_df: pd.DataFrame):\n",
    "        self.data = pd.read_csv(merged_experimental_df)\n",
    "        \n",
    "        # Create amino acid vocabulary\n",
    "        self.aa_vocab = {aa: idx for idx, aa in enumerate('ACDEFGHIKLMNPQRSTVWY')}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Convert sequences to numerical arrays\n",
    "        vh_tensor = torch.tensor([self.aa_vocab[aa] for aa in row['sequences_hc_sequence']], dtype=torch.long)\n",
    "        vl_tensor = torch.tensor([self.aa_vocab[aa] for aa in row['sequences_lc_sequence']], dtype=torch.long)\n",
    "\n",
    "        # Extract target variables\n",
    "        target_columns = ['binding_affinity_kd', 'thermostability_tm1_celsius', 'asec_monomerpct']\n",
    "        targets = row.loc[target_columns].astype(float).values\n",
    "\n",
    "        # Pad the VL to have the same length as the VH\n",
    "        return pad_sequence([vh_tensor, vl_tensor]), torch.tensor(targets)\n",
    "\n",
    "class AntibodyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=20, d_model=128, nhead=8, num_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer for amino acid sequences\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_encoder = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=512,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Transformer encoder\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer=self.pos_encoder,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Output layers\n",
    "        self.fc1 = nn.Linear(d_model * 2, 256)  # *2 because we have VH and VL\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 3)  # 3 outputs: aggregation, KD, Tm1\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 2, seq_length]\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Process VH and VL sequences separately\n",
    "        vh = x[:, 0, :]  # [batch_size, seq_length]\n",
    "        vl = x[:, 1, :]  # [batch_size, seq_length]\n",
    "\n",
    "        # Embed sequences\n",
    "        vh_embedded = self.embedding(vh).transpose(0, 1)  # [seq_length, batch_size, d_model]\n",
    "        vl_embedded = self.embedding(vl).transpose(0, 1)  # [seq_length, batch_size, d_model]\n",
    "\n",
    "        # Pass through transformer\n",
    "        vh_encoded = self.transformer(vh_embedded)  # [seq_length, batch_size, d_model]\n",
    "        vl_encoded = self.transformer(vl_embedded)  # [seq_length, batch_size, d_model]\n",
    "\n",
    "        # Pool sequence dimension\n",
    "        vh_pooled = vh_encoded.mean(dim=0)  # [batch_size, d_model]\n",
    "        vl_pooled = vl_encoded.mean(dim=0)  # [batch_size, d_model]\n",
    "\n",
    "        # Concatenate VH and VL features\n",
    "        combined = torch.cat([vh_pooled, vl_pooled], dim=1)\n",
    "\n",
    "        # Final MLP layers\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-4):\n",
    "    \"\"\"Training loop with validation\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for sequences, targets in train_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in val_loader:\n",
    "                sequences = sequences.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "34e05780-61e3-49c7-8b7c-4662fd374443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Create dataset\n",
    "    dataset = AntibodyDataset('../data/merged_antibody_data.csv')\n",
    "\n",
    "    # Split dataset\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=1\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=1\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = AntibodyTransformer(\n",
    "        vocab_size=20,\n",
    "        d_model=128,\n",
    "        nhead=8,\n",
    "        num_layers=3,\n",
    "        dropout=0.1\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    train_model(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "08331409-7f48-433d-b1d3-b8b73d68e933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training Loss: 4770.9366\n",
      "Validation Loss: 4800.4889\n",
      "Epoch 2/10\n",
      "Training Loss: 4752.8362\n",
      "Validation Loss: 4783.6036\n",
      "Epoch 3/10\n",
      "Training Loss: 4736.4857\n",
      "Validation Loss: 4767.1946\n",
      "Epoch 4/10\n",
      "Training Loss: 4718.8683\n",
      "Validation Loss: 4750.1119\n",
      "Epoch 5/10\n",
      "Training Loss: 4712.3301\n",
      "Validation Loss: 4733.1857\n",
      "Epoch 6/10\n",
      "Training Loss: 4685.8134\n",
      "Validation Loss: 4714.5780\n",
      "Epoch 7/10\n",
      "Training Loss: 4670.3368\n",
      "Validation Loss: 4694.2718\n",
      "Epoch 8/10\n",
      "Training Loss: 4645.7187\n",
      "Validation Loss: 4672.0086\n",
      "Epoch 9/10\n",
      "Training Loss: 4632.4071\n",
      "Validation Loss: 4648.1714\n",
      "Epoch 10/10\n",
      "Training Loss: 4606.8376\n",
      "Validation Loss: 4622.3900\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059b624-08f1-4954-a801-b4f2c70e9607",
   "metadata": {},
   "source": [
    "### Exercise Tasks\n",
    "\n",
    "1.  Data Loading and Assessment\n",
    "    -   Load all three data files\n",
    "    -   Run initial quality assessment\n",
    "    -   Identify key data issues\n",
    "2.  Data Cleaning\n",
    "    -   Implement temperature standardization\n",
    "    -   Clean binding affinity values\n",
    "    -   Handle missing values\n",
    "    -   Standardize units\n",
    "3.  Data Integration\n",
    "    -   Merge datasets\n",
    "    -   Handle duplicate measurements\n",
    "    -   Create final feature matrix\n",
    "4.  ML Model Development\n",
    "    -   Create feature matrix\n",
    "    -   Define target variable\n",
    "    -   Train simple model\n",
    "    -   Evaluate results\n",
    "\n",
    "### Bonus Challenges\n",
    "\n",
    "1.  Advanced Data Cleaning\n",
    "    -   Implement outlier detection\n",
    "    -   Add data validation rules\n",
    "    -   Create data quality reports\n",
    "2.  Feature Engineering\n",
    "    -   Create derived features\n",
    "    -   Implement domain-specific transformations\n",
    "    -   Add sequence-based features\n",
    "3.  Model Improvements\n",
    "    -   Implement cross-validation\n",
    "    -   Try different ML algorithms\n",
    "    -   Add uncertainty quantification\n",
    "\n",
    "### Tips for Success\n",
    "\n",
    "1.  Data Quality\n",
    "    -   Always plot your data\n",
    "    -   Check for outliers\n",
    "    -   Validate units\n",
    "    -   Document assumptions\n",
    "2.  Integration\n",
    "    -   Verify sample IDs\n",
    "    -   Check for duplicates\n",
    "    -   Validate merged data\n",
    "3.  ML Development\n",
    "    -   Start simple\n",
    "    -   Test assumptions\n",
    "    -   Validate results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
