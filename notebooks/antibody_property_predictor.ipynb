{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca1ff0a-23ef-4c69-a1b0-450c70c5396b",
   "metadata": {},
   "source": [
    "# Hands-on Exercise: Processing Legacy Lab Data for ML\n",
    "\n",
    "## Duration: 90 minutes\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this exercise, you'll learn how to: \n",
    "1. Clean and standardize heterogeneous lab data \n",
    "2. Merge data from multiple sources\n",
    "3. Handle missing values and inconsistencies\n",
    "4. Prepare data for ML model training\n",
    "5. Build a basic ML model for antibody property prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85210b1-cbe8-45f4-bea2-fae05052bfe8",
   "metadata": {},
   "source": [
    "### Feature Engineering and ML Preparation\n",
    "\n",
    "You should start by cleaning the data using the `antibody_data_preprocessing` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c42526-5999-4eca-b776-b4fa7f94b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_ml(merged_df):\n",
    "    \"\"\"Prepare merged data for ML modeling\"\"\"\n",
    "    # Create feature matrix\n",
    "    features = [\n",
    "        'binding_affinity_kd',\n",
    "        'thermostability_tm1_celsius',\n",
    "        'asec_monomerpct'\n",
    "    ]\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = merged_df[features].copy()\n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    # Create a simple target variable (should be customized).\n",
    "    # Here we're creating a \"quality score\" combining multiple properties,\n",
    "    # quantifying whether a given sequence is better than the median\n",
    "    y = (\n",
    "        (X['thermostability_tm1_celsius'] > X['thermostability_tm1_celsius'].median()).astype(int) + \n",
    "        (X['asec_monomerpct'] > X['asec_monomerpct'].median()).astype(int) +\n",
    "        (X['binding_affinity_kd'] < X['binding_affinity_kd'].median()).astype(int)\n",
    "    )\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4e6de-24ff-4e3c-a2d2-4214abf7e610",
   "metadata": {},
   "source": [
    "### Build an LSTM to predict the target variable from antibody sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f9d02d-56e3-4e35-8ec7-032f67f8faef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import List, Tuple\n",
    "\n",
    "class AntibodyDataset(Dataset):\n",
    "    def __init__(self, merged_experimental_df: pd.DataFrame):\n",
    "        self.data = merged_experimental_df\n",
    "        #sequences_df.merge(experimental_df, on='Sample ID')\n",
    "\n",
    "        # Create amino acid vocabulary\n",
    "        self.aa_vocab = {aa: idx for idx, aa in enumerate('ACDEFGHIKLMNPQRSTVWY')}\n",
    "'''\n",
    "        # Standardize experimental features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.exp_features = ['KD (nM)', 'Tm1', 'Tm2', '% POI']\n",
    "        self.exp_data = self.scaler.fit_transform(self.data[self.exp_features])\n",
    "'''\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Convert sequences to numerical arrays\n",
    "        vh_tensor = torch.tensor([self.aa_vocab[aa] for aa in row['sequences_hc_sequence']], dtype=torch.long)\n",
    "        vl_tensor = torch.tensor([self.aa_vocab[aa] for aa in row['sequences_lc_sequence']], dtype=torch.long)\n",
    "\n",
    "        # Get experimental features\n",
    "      #  exp_tensor = torch.tensor(self.exp_data[idx], dtype=torch.float)\n",
    "\n",
    "        # Get target variables\n",
    "        y = prepare_for_ml(self.data)[1]\n",
    "\n",
    "        return {\n",
    "            'antibody_id': row['antibody_id'],\n",
    "            'vh': vh_tensor,\n",
    "            'vl': vl_tensor,\n",
    "            'target': y\n",
    "        }\n",
    "\n",
    "class AntibodyPropertyPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size: int = 20, embedding_dim: int = 32,\n",
    "                 hidden_dim: int = 64, num_exp_features: int = 4):\n",
    "        super().__init__()\n",
    "\n",
    "        # Sequence processing\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.vh_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.vl_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Experimental feature processing\n",
    "        self.exp_linear = nn.Linear(num_exp_features, hidden_dim)\n",
    "\n",
    "        # Combined processing\n",
    "        self.combine_layer = nn.Linear(hidden_dim * 4 + hidden_dim, hidden_dim)\n",
    "\n",
    "        # Output layers\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, vh, vl):\n",
    "        # Process VH sequence\n",
    "        vh_emb = self.embedding(vh)\n",
    "        vh_out, (vh_hidden, _) = self.vh_lstm(vh_emb)\n",
    "        vh_feat = torch.cat((vh_hidden[-2,:,:], vh_hidden[-1,:,:]), dim=1)\n",
    "\n",
    "        # Process VL sequence\n",
    "        vl_emb = self.embedding(vl)\n",
    "        vl_out, (vl_hidden, _) = self.vl_lstm(vl_emb)\n",
    "        vl_feat = torch.cat((vl_hidden[-2,:,:], vl_hidden[-1,:,:]), dim=1)\n",
    "\n",
    "        # Process experimental features\n",
    "       # exp_feat = self.exp_linear(exp_features)\n",
    "\n",
    "        # Combine all features\n",
    "        combined = torch.cat([vh_feat, vl_feat], dim=1)\n",
    "        hidden = F.relu(self.combine_layer(combined))\n",
    "\n",
    "        # Generate prediction\n",
    "        output = self.output_layer(hidden)\n",
    "        return output\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader,\n",
    "                num_epochs: int = 100, learning_rate: float = 0.001):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(batch['vh'], batch['vl'])\n",
    "            loss = criterion(output, batch['target'].unsqueeze(1))\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return train_losses\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> Tuple[List[float], List[float]]:\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            output = model(batch['vh'], batch['vl'], batch['exp_features'])\n",
    "            predictions.extend(output.squeeze().tolist())\n",
    "            actuals.extend(batch['target'].tolist())\n",
    "\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25646ce72d574c46",
   "metadata": {},
   "source": [
    "### Train the model on the example data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa6952163908cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    seq_df = pd.read_csv('antibody_sequences.csv')\n",
    "    aggregation_df = pd.read_csv('asec_data.csv')\n",
    "    binding_df = pd.read_csv('binding_data.csv')\n",
    "    stability_df = pd.read_csv('stability_data.csv')\n",
    "\n",
    "    aggregation_df = clean_aggregation_data(aggregation_df)\n",
    "    binding_df = clean_binding_data(binding_df)\n",
    "    stability_df = clean_stability_data(stability_df)\n",
    "\n",
    "    integrated_dataset = integrate_datasets(binding_df, stability_df, aggregation_df)\n",
    "    targets_df = prepare_for_ml(integrated_dataset)[0]\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = AntibodyDataset(seq_df, targets_df)\n",
    "\n",
    "    # Split dataset\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = AntibodyTransformer(\n",
    "        vocab_size=20,\n",
    "        d_model=128,\n",
    "        nhead=8,\n",
    "        num_layers=3,\n",
    "        dropout=0.1\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    train_model(model, train_loader, val_loader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621c034-f61e-45ef-8bb6-34c814e1d4f5",
   "metadata": {},
   "source": [
    "# Transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3e5731c-0b23-405e-ba69-e627058b26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import List, Tuple\n",
    "\n",
    "class AntibodyDataset(Dataset):\n",
    "    def __init__(self, merged_experimental_df: pd.DataFrame):\n",
    "        self.data = pd.read_csv(merged_experimental_df)\n",
    "        #sequences_df.merge(experimental_df, on='Sample ID')\n",
    "\n",
    "        # Create amino acid vocabulary\n",
    "        self.aa_vocab = {aa: idx for idx, aa in enumerate('ACDEFGHIKLMNPQRSTVWY')}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Convert sequences to numerical arrays\n",
    "        vh_tensor = torch.tensor([self.aa_vocab[aa] for aa in row['sequences_hc_sequence']], dtype=torch.long)\n",
    "        vl_tensor = torch.tensor([self.aa_vocab[aa] for aa in row['sequences_lc_sequence']], dtype=torch.long)\n",
    "\n",
    "        # Pad the VL to have the same length as the VH\n",
    "        \n",
    "        \n",
    "        # Get experimental features\n",
    "      #  exp_tensor = torch.tensor(self.exp_data[idx], dtype=torch.float)\n",
    "\n",
    "        # Get target variables\n",
    "        #y = prepare_for_ml(self.data)[1]\n",
    "        targets = self.data[['binding_affinity_kd','thermostability_tm1_celsius','asec_monomerpct']]\n",
    "\n",
    "        return pad_sequence([vh_tensor, vl_tensor]), torch.tensor(targets.values)\n",
    "\n",
    "class AntibodyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=20, d_model=128, nhead=8, num_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer for amino acid sequences\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_encoder = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=512,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Transformer encoder\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer=self.pos_encoder,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Output layers\n",
    "        self.fc1 = nn.Linear(d_model * 2, 256)  # *2 because we have VH and VL\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 4)  # 3 outputs: aggregation, KD, Tm1\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 2, seq_length]\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Process VH and VL sequences separately\n",
    "        vh = x[:, 0, :]  # [batch_size, seq_length]\n",
    "        vl = x[:, 1, :]  # [batch_size, seq_length]\n",
    "\n",
    "        # Embed sequences\n",
    "        vh_embedded = self.embedding(vh).transpose(0, 1)  # [seq_length, batch_size, d_model]\n",
    "        vl_embedded = self.embedding(vl).transpose(0, 1)  # [seq_length, batch_size, d_model]\n",
    "\n",
    "        # Pass through transformer\n",
    "        vh_encoded = self.transformer(vh_embedded)  # [seq_length, batch_size, d_model]\n",
    "        vl_encoded = self.transformer(vl_embedded)  # [seq_length, batch_size, d_model]\n",
    "\n",
    "        # Pool sequence dimension\n",
    "        vh_pooled = vh_encoded.mean(dim=0)  # [batch_size, d_model]\n",
    "        vl_pooled = vl_encoded.mean(dim=0)  # [batch_size, d_model]\n",
    "\n",
    "        # Concatenate VH and VL features\n",
    "        combined = torch.cat([vh_pooled, vl_pooled], dim=1)\n",
    "\n",
    "        # Final MLP layers\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-4):\n",
    "    \"\"\"Training loop with validation\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for sequences, targets in train_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in val_loader:\n",
    "                sequences = sequences.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34e05780-61e3-49c7-8b7c-4662fd374443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Create dataset\n",
    "    dataset = AntibodyDataset('../data/merged_antibody_data.csv')\n",
    "\n",
    "    # Split dataset\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=1\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=1\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = AntibodyTransformer(\n",
    "        vocab_size=20,\n",
    "        d_model=128,\n",
    "        nhead=8,\n",
    "        num_layers=3,\n",
    "        dropout=0.1\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    train_model(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08331409-7f48-433d-b1d3-b8b73d68e933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1, 5, 3])) that is different to the input size (torch.Size([1, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 37\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m AntibodyTransformer(\n\u001b[1;32m     29\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     30\u001b[0m     d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 124\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m    122\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    123\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(sequences)\n\u001b[0;32m--> 124\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    126\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/loss.py:538\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:3383\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3381\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3383\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/functional.py:77\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059b624-08f1-4954-a801-b4f2c70e9607",
   "metadata": {},
   "source": [
    "### Exercise Tasks\n",
    "\n",
    "1.  Data Loading and Assessment\n",
    "    -   Load all three data files\n",
    "    -   Run initial quality assessment\n",
    "    -   Identify key data issues\n",
    "2.  Data Cleaning\n",
    "    -   Implement temperature standardization\n",
    "    -   Clean binding affinity values\n",
    "    -   Handle missing values\n",
    "    -   Standardize units\n",
    "3.  Data Integration\n",
    "    -   Merge datasets\n",
    "    -   Handle duplicate measurements\n",
    "    -   Create final feature matrix\n",
    "4.  ML Model Development\n",
    "    -   Create feature matrix\n",
    "    -   Define target variable\n",
    "    -   Train simple model\n",
    "    -   Evaluate results\n",
    "\n",
    "### Bonus Challenges\n",
    "\n",
    "1.  Advanced Data Cleaning\n",
    "    -   Implement outlier detection\n",
    "    -   Add data validation rules\n",
    "    -   Create data quality reports\n",
    "2.  Feature Engineering\n",
    "    -   Create derived features\n",
    "    -   Implement domain-specific transformations\n",
    "    -   Add sequence-based features\n",
    "3.  Model Improvements\n",
    "    -   Implement cross-validation\n",
    "    -   Try different ML algorithms\n",
    "    -   Add uncertainty quantification\n",
    "\n",
    "### Tips for Success\n",
    "\n",
    "1.  Data Quality\n",
    "    -   Always plot your data\n",
    "    -   Check for outliers\n",
    "    -   Validate units\n",
    "    -   Document assumptions\n",
    "2.  Integration\n",
    "    -   Verify sample IDs\n",
    "    -   Check for duplicates\n",
    "    -   Validate merged data\n",
    "3.  ML Development\n",
    "    -   Start simple\n",
    "    -   Test assumptions\n",
    "    -   Validate results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
