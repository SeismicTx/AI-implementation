{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca1ff0a-23ef-4c69-a1b0-450c70c5396b",
   "metadata": {},
   "source": [
    "# Hands-on Exercise: Processing Legacy Lab Data for ML\n",
    "\n",
    "## Duration: 90 minutes\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this exercise, you'll learn how to: \n",
    "1. Clean and standardize heterogeneous lab data \n",
    "2. Merge data from multiple sources\n",
    "3. Handle missing values and inconsistencies\n",
    "4. Prepare data for ML model training\n",
    "5. Build a basic ML model for antibody property prediction\n",
    "\n",
    "### Data Loading and Initial Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2af9c95b-2caf-47dc-b034-0feae6564371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Load data files\n",
    "def load_binding_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def load_stability_data(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    return df\n",
    "\n",
    "def load_aggregation_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "# Function to assess data quality\n",
    "def assess_data_quality(df, dataset_name):\n",
    "    \"\"\"Analyze common data quality issues\"\"\"\n",
    "    print(f\"\\nAnalyzing {dataset_name}:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"\\nUnique values per column:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"{col}: {df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc93a90-2698-4244-93e7-f0afa3ac638c",
   "metadata": {},
   "source": [
    "### Data Cleaning and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d125d4-5691-4c69-9995-a16ff2fa3e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_binding_data(df):\n",
    "    \"\"\"Clean and standardize binding data\"\"\"\n",
    "    # Handle temperature standardization\n",
    "    def standardize_temp(temp):\n",
    "        if pd.isna(temp):\n",
    "            return None\n",
    "        temp = str(temp).lower()\n",
    "        if 'rt' in temp or 'room' in temp:\n",
    "            return 25.0\n",
    "        return float(re.findall(r'[-+]?\\d*\\.*\\d+', temp)[0])\n",
    "    \n",
    "    # Clean KD values\n",
    "    def clean_kd(kd):\n",
    "        if pd.isna(kd) or kd == 'n.b.':\n",
    "            return None\n",
    "        if isinstance(kd, str) and '<' in kd:\n",
    "            return float(kd.replace('<', ''))\n",
    "        return float(kd)\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    df_clean['Temperature'] = df_clean['Temperature'].apply(standardize_temp)\n",
    "    df_clean['KD (nM)'] = df_clean['KD (nM)'].apply(clean_kd)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def clean_stability_data(df):\n",
    "    \"\"\"Clean and standardize stability data\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    # Ensure numeric Tm values\n",
    "    df_clean['Tm1'] = pd.to_numeric(df_clean['Tm1'], errors='coerce')\n",
    "    df_clean['Tm2'] = pd.to_numeric(df_clean['Tm2'], errors='coerce')\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def clean_aggregation_data(df):\n",
    "    \"\"\"Clean and standardize aggregation data\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    # Convert percentage strings to floats if necessary\n",
    "    df_clean['% POI'] = pd.to_numeric(df_clean['% POI'], errors='coerce')\n",
    "    df_clean['% Aggregate'] = pd.to_numeric(df_clean['% Aggregate'], errors='coerce')\n",
    "    \n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94dc28c-58db-4a5d-8902-012ee60e69c4",
   "metadata": {},
   "source": [
    "### Data Integration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec1ce878-5411-4fb9-8618-f15908eafa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_datasets(binding_df, stability_df, aggregation_df):\n",
    "    \"\"\"Merge all datasets on sample ID\"\"\"\n",
    "    # Standardize sample ID columns\n",
    "    binding_df['Sample_ID'] = binding_df['Sample ID']\n",
    "    stability_df['Sample_ID'] = stability_df['Sample']\n",
    "    aggregation_df['Sample_ID'] = aggregation_df['ID']\n",
    "    \n",
    "    # Get mean values for repeated measurements\n",
    "    binding_summary = binding_df.groupby('Sample_ID')['KD (nM)'].mean().reset_index()\n",
    "    \n",
    "    # Merge all datasets\n",
    "    merged_df = binding_summary.merge(\n",
    "        stability_df[['Sample_ID', 'Tm1', 'Tm2']], \n",
    "        on='Sample_ID', \n",
    "        how='outer'\n",
    "    ).merge(\n",
    "        aggregation_df[['Sample_ID', '% POI', '% Aggregate']], \n",
    "        on='Sample_ID', \n",
    "        how='outer'\n",
    "    )\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85210b1-cbe8-45f4-bea2-fae05052bfe8",
   "metadata": {},
   "source": [
    "### Feature Engineering and ML Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c42526-5999-4eca-b776-b4fa7f94b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_ml(merged_df):\n",
    "    \"\"\"Prepare merged data for ML modeling\"\"\"\n",
    "    # Create feature matrix\n",
    "    features = [\n",
    "        'binding_affinity_kd',\n",
    "        'thermostability_tm1_celsius',\n",
    "        'asec_monomerpct'\n",
    "    ]\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = merged_df[features].copy()\n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    # Create a simple target variable (could be customized)\n",
    "    # Here we're creating a \"quality score\" combining multiple properties,\n",
    "    # quantifying whether a given sequence is better than the median\n",
    "    y = (\n",
    "        (X['thermostability_tm1_celsius'] > X['thermostability_tm1_celsius'].median()).astype(int) + \n",
    "        (X['asec_monomerpct'] > X['asec_monomerpct'].median()).astype(int) +\n",
    "        (X['binding_affinity_kd'] < X['binding_affinity_kd'].median()).astype(int)\n",
    "    )\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4e6de-24ff-4e3c-a2d2-4214abf7e610",
   "metadata": {},
   "source": [
    "### Build an LSTM to predict the target variable from antibody sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f9d02d-56e3-4e35-8ec7-032f67f8faef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import List, Tuple\n",
    "\n",
    "class AntibodyDataset(Dataset):\n",
    "    def __init__(self, merged_experimental_df: pd.DataFrame):\n",
    "        self.data = merged_experimental_df\n",
    "        #sequences_df.merge(experimental_df, on='Sample ID')\n",
    "\n",
    "        # Create amino acid vocabulary\n",
    "        self.aa_vocab = {aa: idx for idx, aa in enumerate('ACDEFGHIKLMNPQRSTVWY')}\n",
    "'''\n",
    "        # Standardize experimental features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.exp_features = ['KD (nM)', 'Tm1', 'Tm2', '% POI']\n",
    "        self.exp_data = self.scaler.fit_transform(self.data[self.exp_features])\n",
    "'''\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Convert sequences to numerical arrays\n",
    "        vh_tensor = torch.tensor([self.aa_vocab[aa] for aa in row['sequences_hc_sequence']], dtype=torch.long)\n",
    "        vl_tensor = torch.tensor([self.aa_vocab[aa] for aa in row['sequences_lc_sequence']], dtype=torch.long)\n",
    "\n",
    "        # Get experimental features\n",
    "      #  exp_tensor = torch.tensor(self.exp_data[idx], dtype=torch.float)\n",
    "\n",
    "        # Get target variables\n",
    "        y = prepare_for_ml(self.data)[1]\n",
    "\n",
    "        return {\n",
    "            'antibody_id': row['antibody_id'],\n",
    "            'vh': vh_tensor,\n",
    "            'vl': vl_tensor,\n",
    "            'target': y\n",
    "        }\n",
    "\n",
    "class AntibodyPropertyPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size: int = 20, embedding_dim: int = 32,\n",
    "                 hidden_dim: int = 64, num_exp_features: int = 4):\n",
    "        super().__init__()\n",
    "\n",
    "        # Sequence processing\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.vh_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.vl_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Experimental feature processing\n",
    "        self.exp_linear = nn.Linear(num_exp_features, hidden_dim)\n",
    "\n",
    "        # Combined processing\n",
    "        self.combine_layer = nn.Linear(hidden_dim * 4 + hidden_dim, hidden_dim)\n",
    "\n",
    "        # Output layers\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, vh, vl):\n",
    "        # Process VH sequence\n",
    "        vh_emb = self.embedding(vh)\n",
    "        vh_out, (vh_hidden, _) = self.vh_lstm(vh_emb)\n",
    "        vh_feat = torch.cat((vh_hidden[-2,:,:], vh_hidden[-1,:,:]), dim=1)\n",
    "\n",
    "        # Process VL sequence\n",
    "        vl_emb = self.embedding(vl)\n",
    "        vl_out, (vl_hidden, _) = self.vl_lstm(vl_emb)\n",
    "        vl_feat = torch.cat((vl_hidden[-2,:,:], vl_hidden[-1,:,:]), dim=1)\n",
    "\n",
    "        # Process experimental features\n",
    "       # exp_feat = self.exp_linear(exp_features)\n",
    "\n",
    "        # Combine all features\n",
    "        combined = torch.cat([vh_feat, vl_feat], dim=1)\n",
    "        hidden = F.relu(self.combine_layer(combined))\n",
    "\n",
    "        # Generate prediction\n",
    "        output = self.output_layer(hidden)\n",
    "        return output\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader,\n",
    "                num_epochs: int = 100, learning_rate: float = 0.001):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(batch['vh'], batch['vl'])\n",
    "            loss = criterion(output, batch['target'].unsqueeze(1))\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return train_losses\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> Tuple[List[float], List[float]]:\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            output = model(batch['vh'], batch['vl'], batch['exp_features'])\n",
    "            predictions.extend(output.squeeze().tolist())\n",
    "            actuals.extend(batch['target'].tolist())\n",
    "\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25646ce72d574c46",
   "metadata": {},
   "source": [
    "### Train the model on the example data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa6952163908cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    seq_df = pd.read_csv('antibody_sequences.csv')\n",
    "    aggregation_df = pd.read_csv('asec_data.csv')\n",
    "    binding_df = pd.read_csv('binding_data.csv')\n",
    "    stability_df = pd.read_csv('stability_data.csv')\n",
    "\n",
    "    aggregation_df = clean_aggregation_data(aggregation_df)\n",
    "    binding_df = clean_binding_data(binding_df)\n",
    "    stability_df = clean_stability_data(stability_df)\n",
    "\n",
    "    integrated_dataset = integrate_datasets(binding_df, stability_df, aggregation_df)\n",
    "    targets_df = prepare_for_ml(integrated_dataset)[0]\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = AntibodyDataset(seq_df, targets_df)\n",
    "\n",
    "    # Split dataset\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = AntibodyTransformer(\n",
    "        vocab_size=20,\n",
    "        d_model=128,\n",
    "        nhead=8,\n",
    "        num_layers=3,\n",
    "        dropout=0.1\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    train_model(model, train_loader, val_loader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059b624-08f1-4954-a801-b4f2c70e9607",
   "metadata": {},
   "source": [
    "### Exercise Tasks\n",
    "\n",
    "1.  Data Loading and Assessment\n",
    "    -   Load all three data files\n",
    "    -   Run initial quality assessment\n",
    "    -   Identify key data issues\n",
    "2.  Data Cleaning\n",
    "    -   Implement temperature standardization\n",
    "    -   Clean binding affinity values\n",
    "    -   Handle missing values\n",
    "    -   Standardize units\n",
    "3.  Data Integration\n",
    "    -   Merge datasets\n",
    "    -   Handle duplicate measurements\n",
    "    -   Create final feature matrix\n",
    "4.  ML Model Development\n",
    "    -   Create feature matrix\n",
    "    -   Define target variable\n",
    "    -   Train simple model\n",
    "    -   Evaluate results\n",
    "\n",
    "### Bonus Challenges\n",
    "\n",
    "1.  Advanced Data Cleaning\n",
    "    -   Implement outlier detection\n",
    "    -   Add data validation rules\n",
    "    -   Create data quality reports\n",
    "2.  Feature Engineering\n",
    "    -   Create derived features\n",
    "    -   Implement domain-specific transformations\n",
    "    -   Add sequence-based features\n",
    "3.  Model Improvements\n",
    "    -   Implement cross-validation\n",
    "    -   Try different ML algorithms\n",
    "    -   Add uncertainty quantification\n",
    "\n",
    "### Tips for Success\n",
    "\n",
    "1.  Data Quality\n",
    "    -   Always plot your data\n",
    "    -   Check for outliers\n",
    "    -   Validate units\n",
    "    -   Document assumptions\n",
    "2.  Integration\n",
    "    -   Verify sample IDs\n",
    "    -   Check for duplicates\n",
    "    -   Validate merged data\n",
    "3.  ML Development\n",
    "    -   Start simple\n",
    "    -   Test assumptions\n",
    "    -   Validate results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
