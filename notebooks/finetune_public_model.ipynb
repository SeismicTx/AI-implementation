{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import esm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class AntibodyDataset(Dataset):\n",
    "    def __init__(self, seq_df, asec_df, binding_df, stability_df, tokenizer, max_length=1024):\n",
    "        \"\"\"\n",
    "        Dataset class for antibody sequences and their properties\n",
    "\n",
    "        Args:\n",
    "            seq_df: DataFrame with VH and VL sequences\n",
    "            asec_df: DataFrame with aggregation data\n",
    "            binding_df: DataFrame with binding data\n",
    "            stability_df: DataFrame with thermal stability data\n",
    "            tokenizer: ESM-2 tokenizer\n",
    "            max_length: Maximum sequence length for padding/truncation\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.process_data(seq_df, asec_df, binding_df, stability_df)\n",
    "\n",
    "    def process_data(self, seq_df, asec_df, binding_df, stability_df):\n",
    "        \"\"\"Merge and process all experimental data\"\"\"\n",
    "        # Process sequences\n",
    "        self.sequences = seq_df.copy()\n",
    "\n",
    "        # Combine VH and VL sequences with a linker\n",
    "        self.sequences['combined_sequence'] = (\n",
    "            self.sequences['VH_seq'] +\n",
    "            'GGGGS' +  # Add a standard linker\n",
    "            self.sequences['VL_seq']\n",
    "        )\n",
    "\n",
    "        # Merge experimental data\n",
    "        merged_data = pd.merge(\n",
    "            self.sequences,\n",
    "            asec_df[['ID', '% Aggregate']],\n",
    "            left_on='Sample ID',\n",
    "            right_on='ID',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        merged_data = pd.merge(\n",
    "            merged_data,\n",
    "            binding_df[['Sample ID', 'KD (nM)']],\n",
    "            on='Sample ID',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        merged_data = pd.merge(\n",
    "            merged_data,\n",
    "            stability_df[['Sample', 'Tm1', 'Tm2']],\n",
    "            left_on='Sample ID',\n",
    "            right_on='Sample',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Convert and clean numeric columns\n",
    "        merged_data['% Aggregate'] = pd.to_numeric(merged_data['% Aggregate'], errors='coerce')\n",
    "        merged_data['KD (nM)'] = pd.to_numeric(merged_data['KD (nM)'], errors='coerce')\n",
    "\n",
    "        # Handle missing values\n",
    "        self.processed_data = merged_data.fillna({\n",
    "            '% Aggregate': merged_data['% Aggregate'].mean(),\n",
    "            'KD (nM)': merged_data['KD (nM)'].mean(),\n",
    "            'Tm1': merged_data['Tm1'].mean(),\n",
    "            'Tm2': merged_data['Tm2'].mean()\n",
    "        })\n",
    "\n",
    "        # Scale targets\n",
    "        self.scaler = StandardScaler()\n",
    "        self.targets = self.scaler.fit_transform(\n",
    "            self.processed_data[['% Aggregate', 'KD (nM)', 'Tm1', 'Tm2']].values\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.processed_data.iloc[idx]['combined_sequence']\n",
    "\n",
    "        # Tokenize sequence\n",
    "        inputs = self.tokenizer(\n",
    "            sequence,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Get target values\n",
    "        targets = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'targets': targets\n",
    "        }\n",
    "\n",
    "class ESM2ForAntibodyPrediction(nn.Module):\n",
    "    def __init__(self, model_name=\"facebook/esm2_t33_650M_UR50D\", num_labels=4):\n",
    "        \"\"\"\n",
    "        Fine-tuning ESM-2 for antibody property prediction\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the ESM-2 model to use\n",
    "            num_labels: Number of properties to predict\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Load pretrained ESM-2 model\n",
    "        self.esm2 = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Freeze some layers (optional)\n",
    "        self.freeze_layers(num_layers_to_freeze=8)\n",
    "\n",
    "        # Add prediction head\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.esm2.config.hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "\n",
    "    def freeze_layers(self, num_layers_to_freeze):\n",
    "        \"\"\"Freeze the first n transformer layers\"\"\"\n",
    "        modules_to_freeze = [\n",
    "            self.esm2.embeddings,\n",
    "            *self.esm2.encoder.layer[:num_layers_to_freeze]\n",
    "        ]\n",
    "        for module in modules_to_freeze:\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get ESM-2 embeddings\n",
    "        outputs = self.esm2(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        # Use CLS token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        # Predict properties\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=2e-5):\n",
    "    \"\"\"Training loop with validation\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Initialize optimizer with weight decay\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [\n",
    "            {'params': model.classifier.parameters(), 'lr': learning_rate},\n",
    "            {'params': model.esm2.parameters(), 'lr': learning_rate/10}\n",
    "        ],\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=num_epochs\n",
    "    )\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                targets = batch['targets'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_esm2_model.pth')\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "        print(f'Learning Rate: {scheduler.get_last_lr()[0]:.2e}')\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    seq_df = pd.read_csv('antibody_sequences.csv')\n",
    "    asec_df = pd.read_csv('asec_data.csv')\n",
    "    binding_df = pd.read_csv('binding_data.csv')\n",
    "    stability_df = pd.read_csv('stability_data.csv')\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = AntibodyDataset(\n",
    "        seq_df,\n",
    "        asec_df,\n",
    "        binding_df,\n",
    "        stability_df,\n",
    "        tokenizer\n",
    "    )\n",
    "\n",
    "    # Split dataset\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=8,  # Smaller batch size due to model size\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = ESM2ForAntibodyPrediction()\n",
    "\n",
    "    # Train model\n",
    "    train_model(model, train_loader, val_loader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
