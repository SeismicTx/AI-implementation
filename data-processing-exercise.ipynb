{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca1ff0a-23ef-4c69-a1b0-450c70c5396b",
   "metadata": {},
   "source": [
    "# Hands-on Exercise: Processing Legacy Lab Data for ML\n",
    "\n",
    "## Duration: 90 minutes\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this exercise, you'll learn how to: \n",
    "1. Clean and standardize heterogeneous lab data \n",
    "2. Merge data from multiple sources\n",
    "3. Handle missing values and inconsistencies\n",
    "4. Prepare data for ML model training\n",
    "5. Build a simple ML model for antibody property prediction\n",
    "\n",
    "### Part 1: Data Loading and Initial Assessment (20 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af9c95b-2caf-47dc-b034-0feae6564371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Load data files\n",
    "def load_binding_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def load_stability_data(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    return df\n",
    "\n",
    "def load_aggregation_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "# Function to assess data quality\n",
    "def assess_data_quality(df, dataset_name):\n",
    "    \"\"\"Analyze common data quality issues\"\"\"\n",
    "    print(f\"\\nAnalyzing {dataset_name}:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"\\nUnique values per column:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"{col}: {df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc93a90-2698-4244-93e7-f0afa3ac638c",
   "metadata": {},
   "source": [
    "### Part 2: Data Cleaning and Standardization (30 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d125d4-5691-4c69-9995-a16ff2fa3e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_binding_data(df):\n",
    "    \"\"\"Clean and standardize binding data\"\"\"\n",
    "    # Handle temperature standardization\n",
    "    def standardize_temp(temp):\n",
    "        if pd.isna(temp):\n",
    "            return None\n",
    "        temp = str(temp).lower()\n",
    "        if 'rt' in temp or 'room' in temp:\n",
    "            return 25.0\n",
    "        return float(re.findall(r'[-+]?\\d*\\.*\\d+', temp)[0])\n",
    "    \n",
    "    # Clean KD values\n",
    "    def clean_kd(kd):\n",
    "        if pd.isna(kd) or kd == 'n.b.':\n",
    "            return None\n",
    "        if isinstance(kd, str) and '<' in kd:\n",
    "            return float(kd.replace('<', ''))\n",
    "        return float(kd)\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    df_clean['Temperature'] = df_clean['Temperature'].apply(standardize_temp)\n",
    "    df_clean['KD (nM)'] = df_clean['KD (nM)'].apply(clean_kd)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def clean_stability_data(df):\n",
    "    \"\"\"Clean and standardize stability data\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    # Ensure numeric Tm values\n",
    "    df_clean['Tm1'] = pd.to_numeric(df_clean['Tm1'], errors='coerce')\n",
    "    df_clean['Tm2'] = pd.to_numeric(df_clean['Tm2'], errors='coerce')\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def clean_aggregation_data(df):\n",
    "    \"\"\"Clean and standardize aggregation data\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    # Convert percentage strings to floats if necessary\n",
    "    df_clean['% Monomer'] = pd.to_numeric(df_clean['% Monomer'], errors='coerce')\n",
    "    df_clean['% Aggregate'] = pd.to_numeric(df_clean['% Aggregate'], errors='coerce')\n",
    "    \n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94dc28c-58db-4a5d-8902-012ee60e69c4",
   "metadata": {},
   "source": [
    "### Part 3: Data Integration (20 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec1ce878-5411-4fb9-8618-f15908eafa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_datasets(binding_df, stability_df, aggregation_df):\n",
    "    \"\"\"Merge all datasets on sample ID\"\"\"\n",
    "    # Standardize sample ID columns\n",
    "    binding_df['Sample_ID'] = binding_df['Sample ID']\n",
    "    stability_df['Sample_ID'] = stability_df['Sample']\n",
    "    aggregation_df['Sample_ID'] = aggregation_df['ID']\n",
    "    \n",
    "    # Get mean values for repeated measurements\n",
    "    binding_summary = binding_df.groupby('Sample_ID')['KD (nM)'].mean().reset_index()\n",
    "    \n",
    "    # Merge all datasets\n",
    "    merged_df = binding_summary.merge(\n",
    "        stability_df[['Sample_ID', 'Tm1', 'Tm2']], \n",
    "        on='Sample_ID', \n",
    "        how='outer'\n",
    "    ).merge(\n",
    "        aggregation_df[['Sample_ID', '% Monomer', '% Aggregate']], \n",
    "        on='Sample_ID', \n",
    "        how='outer'\n",
    "    )\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85210b1-cbe8-45f4-bea2-fae05052bfe8",
   "metadata": {},
   "source": [
    "### Part 4: Feature Engineering and ML Preparation (20 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c42526-5999-4eca-b776-b4fa7f94b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_ml(merged_df):\n",
    "    \"\"\"Prepare merged data for ML modeling\"\"\"\n",
    "    # Create feature matrix\n",
    "    features = [\n",
    "        'KD (nM)',\n",
    "        'Tm1',\n",
    "        'Tm2',\n",
    "        '% Monomer'\n",
    "    ]\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = merged_df[features].copy()\n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    # Create a simple target variable (could be customized)\n",
    "    # Here we're creating a \"quality score\" combining multiple properties\n",
    "    y = (\n",
    "        (X['Tm1'] > X['Tm1'].median()).astype(int) + \n",
    "        (X['% Monomer'] > X['% Monomer'].median()).astype(int) +\n",
    "        (X['KD (nM)'] < X['KD (nM)'].median()).astype(int)\n",
    "    )\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059b624-08f1-4954-a801-b4f2c70e9607",
   "metadata": {},
   "source": [
    "### Exercise Tasks\n",
    "\n",
    "1.  Data Loading and Assessment\n",
    "    -   Load all three data files\n",
    "    -   Run initial quality assessment\n",
    "    -   Identify key data issues\n",
    "2.  Data Cleaning\n",
    "    -   Implement temperature standardization\n",
    "    -   Clean binding affinity values\n",
    "    -   Handle missing values\n",
    "    -   Standardize units\n",
    "3.  Data Integration\n",
    "    -   Merge datasets\n",
    "    -   Handle duplicate measurements\n",
    "    -   Create final feature matrix\n",
    "4.  ML Model Development\n",
    "    -   Create feature matrix\n",
    "    -   Define target variable\n",
    "    -   Train simple model\n",
    "    -   Evaluate results\n",
    "\n",
    "### Bonus Challenges\n",
    "\n",
    "1.  Advanced Data Cleaning\n",
    "    -   Implement outlier detection\n",
    "    -   Add data validation rules\n",
    "    -   Create data quality reports\n",
    "2.  Feature Engineering\n",
    "    -   Create derived features\n",
    "    -   Implement domain-specific transformations\n",
    "    -   Add sequence-based features\n",
    "3.  Model Improvements\n",
    "    -   Implement cross-validation\n",
    "    -   Try different ML algorithms\n",
    "    -   Add uncertainty quantification\n",
    "\n",
    "### Tips for Success\n",
    "\n",
    "1.  Data Quality\n",
    "    -   Always plot your data\n",
    "    -   Check for outliers\n",
    "    -   Validate units\n",
    "    -   Document assumptions\n",
    "2.  Integration\n",
    "    -   Verify sample IDs\n",
    "    -   Check for duplicates\n",
    "    -   Validate merged data\n",
    "3.  ML Development\n",
    "    -   Start simple\n",
    "    -   Test assumptions\n",
    "    -   Validate results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
